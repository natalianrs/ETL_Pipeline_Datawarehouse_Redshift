# Extracting data from S3 Buckets and loading to Redshift   
## Project Structure ðŸ› 
1. Creating AWS S3 buckets to store data;
2. Creating a redshift cluster using boto3;
3. Creating IAM user and access credentials;
4. Dimensional data modeling 
5. Build a etl pipeline to extract data from S3 bucket and load data into redshift datawarehouse 
6. Validating data and running queries

## Repository Structure ðŸ“‚ <br>

    |--- dwh.cfg                            # aws access keys  
    |--- creating_redshift_cluster_boto3    # routines to connect to redshift and create a cluster  
    |--- sql_queries.py                     # sql statements CREATE/INSERT/DROP  
    |--- create_tables.py                   # connects to redshift and creates facts and dimensions tables  
    |--- etl.py                             # connects to redshift, extracts/tranforms/loads data to datawarehouse  

## Data Modeling ðŸ’¾
The data resides in two different s3 buckets.  
Two distinct sources were used:   
- Sub dataset of ["Milion Song Dataset"](http://millionsongdataset.com/) contains songs info and metadata.
- Logs and info generated by ['Eventsim'](https://github.com/Interana/eventsim) to simulate real users usage.

**The data were modeled with the goal of optimizing the SQL query process**. <br>
- A denormalized star schema was created. 
- The fact table"songplays" gives us the info of the song listened in a given activity log. 
- The dimensional tables provides us with additional details of the facts table.
![img](https://raw.githubusercontent.com/natalianrs/ETL_Pipeline_PostgreSQL/main/data_model_postgre.png)


## Data Pipeline âš™
The file etl.py is the script for ETL pipeline to extract, transform and load data into the datawarehouse.  <br>
The file is structure in these main objectives:<br>
- Importing sql_queries.py as module;<br>
- Connecting to redshift cluster;<br>
- Loading  staging tables  in redshift;<br>
- Inserting data into tables  in redshift;<br>
	

## How to run this project ðŸ’¡
	1. Login to AWS Account 
	2. Create a IAM user and save access credentials 
	3. Create a s3 bucket to store initial data;
	4. Create a cluster in redshift using boto3 
		- Save the AWS Access key
		- Load DWH Params from a file
		- Create clients for IAM, EC2, S3, and Redshift
		- Check out the sample data sources on S3
		- Create an IAM ROLE
		- Create the Redshift Cluster
		- Describe the cluster to see its status
		- Take note of the cluster endpoint and role ARN
		- Open an incoming TCP port to access the cluster ednpoint
		- Make sure you can connect to the cluster
	5. Run `python create_tables.py`  to connect to cluster and create tables
	6. Run `python etl.py` to extract data and load to redshift tables.
	7. Use `query editor`in AWS to query database

ðŸŽ…
